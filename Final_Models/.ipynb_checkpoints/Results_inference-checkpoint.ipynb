{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfdc12b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel                      as nib\n",
    "import nibabel.freesurfer.mghformat as mgh\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(0)\n",
    "np.random.seed(0)\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ab67846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0\n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(tf.config.list_physical_devices())\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "629dd833",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import ndimage\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "\n",
    "def read_mgz_file(filepath):\n",
    "    \"\"\"Read and load volume\"\"\"\n",
    "    # Read file\n",
    "    scan = mgh.load(filepath)\n",
    "    # Get raw data\n",
    "    scan = scan.get_fdata()\n",
    "    return scan\n",
    "\n",
    "def convertersize(volume):\n",
    "    \"\"\"convertersize the volume\"\"\"\n",
    "    min = -1000\n",
    "    max = 400\n",
    "    volume[volume < min] = min\n",
    "    volume[volume > max] = max\n",
    "    volume = (volume - min) / (max - min)\n",
    "    volume = volume.astype(\"float32\")\n",
    "    return volume\n",
    "\n",
    "def resize_volume(img):\n",
    "    \"\"\"Resize across z-axis\"\"\"\n",
    "    # Set the desired depth\n",
    "    desired_depth = 256\n",
    "    desired_width = 256\n",
    "    desired_height = 256\n",
    "    # Get current depth\n",
    "    current_depth = img.shape[-1]\n",
    "    current_width = img.shape[0]\n",
    "    current_height = img.shape[1]\n",
    "    # Compute depth factor\n",
    "    depth = current_depth / desired_depth\n",
    "    width = current_width / desired_width\n",
    "    height = current_height / desired_height\n",
    "    depth_factor = 1 / depth\n",
    "    width_factor = 1 / width\n",
    "    height_factor = 1 / height\n",
    "    # Rotate\n",
    "    img = ndimage.rotate(img, 270, reshape=False)\n",
    "    # Resize across z-axis\n",
    "    img = ndimage.zoom(img, (width_factor, height_factor, depth_factor), order=1)\n",
    "    return img\n",
    "\n",
    "def crop_image(img):\n",
    "    # Find first and last slices that contain parts of the brain\n",
    "    blank = img[0].sum()\n",
    "    x = []\n",
    "    for i in range(len(img)):\n",
    "        if img[i,:,:].sum() != blank:\n",
    "            x.append(i)\n",
    "    y = []\n",
    "    for i in range(len(img)):\n",
    "        if img[:,i,:].sum() != blank:\n",
    "            y.append(i)\n",
    "    z = []\n",
    "    for i in range(len(img)):\n",
    "        if img[:,:,i].sum() != blank:\n",
    "            z.append(i)\n",
    "    # Use these to crop the 3D images\n",
    "    img = np.squeeze(img[min(x):max(x), min(y):max(y), min(z):max(z)])\n",
    "    \n",
    "    # Set the desired depth\n",
    "    desired_depth = 128\n",
    "    desired_width = 128\n",
    "    desired_height = 128\n",
    "    # Get current depth\n",
    "    current_depth = img.shape[-1]\n",
    "    current_width = img.shape[0]\n",
    "    current_height = img.shape[1]\n",
    "    # Compute depth factor\n",
    "    depth = current_depth / desired_depth\n",
    "    width = current_width / desired_width\n",
    "    height = current_height / desired_height\n",
    "    depth_factor = 1 / depth\n",
    "    width_factor = 1 / width\n",
    "    height_factor = 1 / height\n",
    "    # Resize across z-axis\n",
    "    img = ndimage.zoom(img, (width_factor, height_factor, depth_factor), order=1)\n",
    "    return img\n",
    "\n",
    "def create_patches(img):\n",
    "    patches=[]\n",
    "    for i in [0,32,64]:\n",
    "        for j in [0,32,64]:\n",
    "            for k in [0,32,64]:\n",
    "                patch = np.squeeze(img[i:(i+64), j:(j+64), k:(k+64)])\n",
    "                # [0, 1] normalization\n",
    "                patch = patch/(patch.max()/1)\n",
    "                patch = patch - 1\n",
    "                patch = abs(patch)\n",
    "                patch = patch/(patch.max()/1)\n",
    "                patch = abs(patch-1)\n",
    "                patches.append(patch)\n",
    "    return patches\n",
    "\n",
    "def process_scan(path):\n",
    "    \"\"\"Read and resize volume\"\"\"\n",
    "    # Read scan\n",
    "    volume = read_mgz_file(path)\n",
    "    #  convertersize\n",
    "    volume = convertersize(volume)\n",
    "    # Resize width, height and depth\n",
    "    volume = resize_volume(volume)\n",
    "    # Crop 3D image\n",
    "    volume = crop_image(volume)\n",
    "    # Create patches\n",
    "    volume = create_patches(volume)\n",
    "    return volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6bd8bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder \"Train_CN\" consist of subjects who remained cognitively converters for the whole study.\n",
    "Train_CN_paths = [\n",
    "    os.path.join(os.getcwd(), \"Train_CN\", x)\n",
    "    for x in os.listdir(\"Train_CN\")\n",
    "]\n",
    "\n",
    "# Folder \"Val_CN\" consist of subjects who remained cognitively converters for the whole study.\n",
    "Val_CN_paths = [\n",
    "    os.path.join(os.getcwd(), \"Val_CN\", x)\n",
    "    for x in os.listdir(\"Val_CN\")\n",
    "]\n",
    "\n",
    "# Folder \"Test_CN\" consist of subjects who remained cognitively converters for the whole study.\n",
    "Test_CN_paths = [\n",
    "    os.path.join(os.getcwd(), \"Test_CN\", x)\n",
    "    for x in os.listdir(\"Test_CN\")\n",
    "]\n",
    "\n",
    "\n",
    "# Folder \"Train_Converters\" consist of subjects who converted from CN to MCI > 0.\n",
    "Train_Converters_paths = [\n",
    "    os.path.join(os.getcwd(), \"Train_Converters\", x)\n",
    "    for x in os.listdir(\"Train_Converters\")\n",
    "]\n",
    "\n",
    "# Folder \"Val_Converters\" consist of subjects who converted from CN to MCI > 0.\n",
    "Val_Converters_paths = [\n",
    "    os.path.join(os.getcwd(), \"Val_Converters\", x)\n",
    "    for x in os.listdir(\"Val_Converters\")\n",
    "]\n",
    "\n",
    "# Folder \"Test_Converters\" consist of subjects who converted from CN to MCI > 0.\n",
    "Test_Converters_paths = [\n",
    "    os.path.join(os.getcwd(), \"Test_Converters\", x)\n",
    "    for x in os.listdir(\"Test_Converters\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c7768e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and process the scans.\n",
    "# Each scan is resized across height, width, and depth and rescaled.\n",
    "Train_CN_scans = np.array([process_scan(path) for path in Train_CN_paths])\n",
    "Val_CN_scans = np.array([process_scan(path) for path in Val_CN_paths])\n",
    "Test_CN_scans = np.array([process_scan(path) for path in Test_CN_paths])\n",
    "\n",
    "Train_Converters_scans = np.array([process_scan(path) for path in Train_Converters_paths])\n",
    "Val_Converters_scans = np.array([process_scan(path) for path in Val_Converters_paths])\n",
    "Test_Converters_scans = np.array([process_scan(path) for path in Test_Converters_paths])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68020ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the converters assign 1, for the CN subjects assign 0.\n",
    "Train_CN_labels = np.array([0 for _ in range(len(Train_CN_scans))])\n",
    "Val_CN_labels = np.array([0 for _ in range(len(Val_CN_scans))])\n",
    "Test_CN_labels = np.array([0 for _ in range(len(Test_CN_scans))])\n",
    "\n",
    "Train_Converters_labels = np.array([1 for _ in range(len(Train_Converters_scans))])\n",
    "Val_Converters_labels = np.array([1 for _ in range(len(Val_Converters_scans))])\n",
    "Test_Converters_labels = np.array([1 for _ in range(len(Test_Converters_scans))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "baaf45c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "scan_id_train = []\n",
    "for scan_path in Train_CN_paths:\n",
    "    scan_id_train.append(scan_path.split('\\\\')[-1].split('.')[0])\n",
    "for scan_path in Train_Converters_paths:\n",
    "    scan_id_train.append(scan_path.split('\\\\')[-1].split('.')[0])\n",
    "\n",
    "scan_id_val = []\n",
    "for scan_path in Val_CN_paths:\n",
    "    scan_id_val.append(scan_path.split('\\\\')[-1].split('.')[0])\n",
    "for scan_path in Val_Converters_paths:\n",
    "    scan_id_val.append(scan_path.split('\\\\')[-1].split('.')[0])\n",
    "    \n",
    "scan_id_test = []\n",
    "for scan_path in Test_CN_paths:\n",
    "    scan_id_test.append(scan_path.split('\\\\')[-1].split('.')[0])\n",
    "for scan_path in Test_Converters_paths:\n",
    "    scan_id_test.append(scan_path.split('\\\\')[-1].split('.')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6b21fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ResNet18' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5472/477895046.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test_rgb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[0mnew_row\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'patch'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'type'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m'test'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'scan_id'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mscan_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'actual'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'pred'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mprediction\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m         \u001b[0mResNet18\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mResNet18\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_row\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ResNet18' is not defined"
     ]
    }
   ],
   "source": [
    "column_names = [\"patch\", \"type\", \"scan_id\", \"actual\", \"pred\"]\n",
    "results_DenseNet121 = pd.DataFrame(columns = column_names)\n",
    "results_ResNet18 = pd.DataFrame(columns = column_names)\n",
    "results_ResNet50 = pd.DataFrame(columns = column_names)\n",
    "results_SEResNet50 = pd.DataFrame(columns = column_names)\n",
    "results_SEResNeXt50 = pd.DataFrame(columns = column_names)\n",
    "\n",
    "models = ['DenseNet121', 'ResNet50', 'SEResNet50', 'SEResNeXt50']\n",
    "\n",
    "for i in range(27):\n",
    "    print(i)\n",
    "    # Read the patches from the processed scans.\n",
    "    Train_CN_patches = np.array([scan[i,:,:,:] for scan in Train_CN_scans])\n",
    "    Val_CN_patches = np.array([scan[i,:,:,:] for scan in Val_CN_scans])\n",
    "    Test_CN_patches = np.array([scan[i,:,:,:] for scan in Test_CN_scans])\n",
    "    Train_Converters_patches = np.array([scan[i,:,:,:] for scan in Train_Converters_scans])\n",
    "    Val_Converters_patches = np.array([scan[i,:,:,:] for scan in Val_Converters_scans])\n",
    "    Test_Converters_patches = np.array([scan[i,:,:,:] for scan in Test_Converters_scans])\n",
    "\n",
    "    # Create x and y values for train, validation and test sets\n",
    "    x_train = np.concatenate((Train_CN_patches, Train_Converters_patches), axis=0)\n",
    "    x_val = np.concatenate((Val_CN_patches, Val_Converters_patches), axis=0)\n",
    "    x_test = np.concatenate((Test_CN_patches, Test_Converters_patches), axis=0)\n",
    "    y_train = np.concatenate((Train_CN_labels, Train_Converters_labels), axis=0)\n",
    "    y_val = np.concatenate((Val_CN_labels, Val_Converters_labels), axis=0)\n",
    "    y_test = np.concatenate((Test_CN_labels, Test_Converters_labels), axis=0)\n",
    "\n",
    "    # Convert image data to RGB (3 channels) so ImageNet weights work\n",
    "    x_train_rgb = np.repeat(x_train[..., np.newaxis], 3, -1)\n",
    "    x_val_rgb = np.repeat(x_val[..., np.newaxis], 3, -1)\n",
    "    x_test_rgb = np.repeat(x_test[..., np.newaxis], 3, -1)\n",
    "    \n",
    "    \n",
    "    # DenseNet121\n",
    "    # Read the trained model\n",
    "    model = keras.models.load_model(str(\"DenseNet121/3d_image_classification\"+str(i)+\".h5\"))\n",
    "    \n",
    "    # Make predictions using the trained model\n",
    "    for x in range(len(x_train_rgb)):\n",
    "        scan_id = scan_id_train[x]\n",
    "        prediction = model.predict(np.expand_dims(x_train_rgb[x], axis=0))[0]\n",
    "        new_row = {'patch':i, 'type':'train', 'scan_id':scan_id, 'actual':y_train[x], 'pred':prediction[0]}\n",
    "        results_DenseNet121 = results_DenseNet121.append(new_row, ignore_index=True)\n",
    "    \n",
    "    for x in range(len(x_val_rgb)):\n",
    "        scan_id = scan_id_val[x]\n",
    "        prediction = model.predict(np.expand_dims(x_val_rgb[x], axis=0))[0]\n",
    "        new_row = {'patch':i, 'type':'val', 'scan_id':scan_id, 'actual':y_val[x], 'pred':prediction[0]}\n",
    "        results_DenseNet121 = results_DenseNet121.append(new_row, ignore_index=True)\n",
    "        \n",
    "    for x in range(len(x_test_rgb)):\n",
    "        scan_id = scan_id_test[x]\n",
    "        prediction = model.predict(np.expand_dims(x_test_rgb[x], axis=0))[0]\n",
    "        new_row = {'patch':i, 'type':'test', 'scan_id':scan_id, 'actual':y_test[x], 'pred':prediction[0]}\n",
    "        results_DenseNet121 = results_DenseNet121.append(new_row, ignore_index=True)\n",
    "\n",
    "    \n",
    "    # ResNet18\n",
    "    # Read the trained model\n",
    "    model = keras.models.load_model(str(\"ResNet18/3d_image_classification\"+str(i)+\".h5\"))\n",
    "    \n",
    "    # Make predictions using the trained model\n",
    "    for x in range(len(x_train_rgb)):\n",
    "        scan_id = scan_id_train[x]\n",
    "        prediction = model.predict(np.expand_dims(x_train_rgb[x], axis=0))[0]\n",
    "        new_row = {'patch':i, 'type':'train', 'scan_id':scan_id, 'actual':y_train[x], 'pred':prediction[0]}\n",
    "        results_ResNet18 = results_ResNet18.append(new_row, ignore_index=True)\n",
    "    \n",
    "    for x in range(len(x_val_rgb)):\n",
    "        scan_id = scan_id_train[x]\n",
    "        prediction = model.predict(np.expand_dims(x_val_rgb[x], axis=0))[0]\n",
    "        new_row = {'patch':i, 'type':'val', 'scan_id':scan_id, 'actual':y_val[x], 'pred':prediction[0]}\n",
    "        results_ResNet18 = results_ResNet18.append(new_row, ignore_index=True)\n",
    "        \n",
    "    for x in range(len(x_test_rgb)):\n",
    "        scan_id = scan_id_train[x]\n",
    "        prediction = model.predict(np.expand_dims(x_test_rgb[x], axis=0))[0]\n",
    "        new_row = {'patch':i, 'type':'test', 'scan_id':scan_id, 'actual':y_test[x], 'pred':prediction[0]}\n",
    "        results_ResNet18 = results_ResNet18.append(new_row, ignore_index=True)\n",
    "    \n",
    "    \n",
    "    # ResNet50\n",
    "    # Read the trained model\n",
    "    model = keras.models.load_model(str(\"ResNet50/3d_image_classification\"+str(i)+\".h5\"))\n",
    "    \n",
    "    # Make predictions using the trained model\n",
    "    for x in range(len(x_train_rgb)):\n",
    "        scan_id = scan_id_train[x]\n",
    "        prediction = model.predict(np.expand_dims(x_train_rgb[x], axis=0))[0]\n",
    "        new_row = {'patch':i, 'type':'train', 'scan_id':scan_id, 'actual':y_train[x], 'pred':prediction[0]}\n",
    "        results_ResNet50 = results_ResNet50.append(new_row, ignore_index=True)\n",
    "    \n",
    "    for x in range(len(x_val_rgb)):\n",
    "        scan_id = scan_id_train[x]\n",
    "        prediction = model.predict(np.expand_dims(x_val_rgb[x], axis=0))[0]\n",
    "        new_row = {'patch':i, 'type':'val', 'scan_id':scan_id, 'actual':y_val[x], 'pred':prediction[0]}\n",
    "        results_ResNet50 = results_ResNet50.append(new_row, ignore_index=True)\n",
    "        \n",
    "    for x in range(len(x_test_rgb)):\n",
    "        scan_id = scan_id_train[x]\n",
    "        prediction = model.predict(np.expand_dims(x_test_rgb[x], axis=0))[0]\n",
    "        new_row = {'patch':i, 'type':'test', 'scan_id':scan_id, 'actual':y_test[x], 'pred':prediction[0]}\n",
    "        results_ResNet50 = results_ResNet50.append(new_row, ignore_index=True)\n",
    "        \n",
    "    # SEResNet50\n",
    "    # Read the trained model\n",
    "    model = keras.models.load_model(str(\"SEResNet50/3d_image_classification\"+str(i)+\".h5\"))\n",
    "    \n",
    "    # Make predictions using the trained model\n",
    "    for x in range(len(x_train_rgb)):\n",
    "        scan_id = scan_id_train[x]\n",
    "        prediction = model.predict(np.expand_dims(x_train_rgb[x], axis=0))[0]\n",
    "        new_row = {'patch':i, 'type':'train', 'scan_id':scan_id, 'actual':y_train[x], 'pred':prediction[0]}\n",
    "        results_SEResNet50 = results_SEResNet50.append(new_row, ignore_index=True)\n",
    "    \n",
    "    for x in range(len(x_val_rgb)):\n",
    "        scan_id = scan_id_train[x]\n",
    "        prediction = model.predict(np.expand_dims(x_val_rgb[x], axis=0))[0]\n",
    "        new_row = {'patch':i, 'type':'val', 'scan_id':scan_id, 'actual':y_val[x], 'pred':prediction[0]}\n",
    "        results_SEResNet50 = results_SEResNet50.append(new_row, ignore_index=True)\n",
    "        \n",
    "    for x in range(len(x_test_rgb)):\n",
    "        scan_id = scan_id_train[x]\n",
    "        prediction = model.predict(np.expand_dims(x_test_rgb[x], axis=0))[0]\n",
    "        new_row = {'patch':i, 'type':'test', 'scan_id':scan_id, 'actual':y_test[x], 'pred':prediction[0]}\n",
    "        results_SEResNet50 = results_SEResNet50.append(new_row, ignore_index=True)\n",
    "\n",
    "    \n",
    "    # SEResNeXt50\n",
    "    # Read the trained model\n",
    "    model = keras.models.load_model(str(\"SEResNeXt50/3d_image_classification\"+str(i)+\".h5\"))\n",
    "\n",
    "    # Make predictions using the trained model\n",
    "    for x in range(len(x_train_rgb)):\n",
    "        scan_id = scan_id_train[x]\n",
    "        prediction = model.predict(np.expand_dims(x_train_rgb[x], axis=0))[0]\n",
    "        new_row = {'patch':i, 'type':'train', 'scan_id':scan_id, 'actual':y_train[x], 'pred':prediction[0]}\n",
    "        results_SEResNeXt50 = results_SEResNeXt50.append(new_row, ignore_index=True)\n",
    "\n",
    "    for x in range(len(x_val_rgb)):\n",
    "        scan_id = scan_id_train[x]\n",
    "        prediction = model.predict(np.expand_dims(x_val_rgb[x], axis=0))[0]\n",
    "        new_row = {'patch':i, 'type':'val', 'scan_id':scan_id, 'actual':y_val[x], 'pred':prediction[0]}\n",
    "        results_SEResNeXt50 = results_SEResNeXt50.append(new_row, ignore_index=True)\n",
    "\n",
    "    for x in range(len(x_test_rgb)):\n",
    "        scan_id = scan_id_train[x]\n",
    "        prediction = model.predict(np.expand_dims(x_test_rgb[x], axis=0))[0]\n",
    "        new_row = {'patch':i, 'type':'test', 'scan_id':scan_id, 'actual':y_test[x], 'pred':prediction[0]}\n",
    "        results_SEResNeXt50 = results_SEResNeXt50.append(new_row, ignore_index=True)\n",
    "\n",
    "        \n",
    "# Store our prediction results\n",
    "results_DenseNet121.to_csv('Results/results_DenseNet121.csv', index=False)\n",
    "results_ResNet50.to_csv('Results/results_ResNet50.csv', index=False)\n",
    "results_ResNet18.to_csv('Results/results_ResNet18.csv', index=False)\n",
    "results_SEResNet50.to_csv('Results/results_SEResNet50.csv', index=False)\n",
    "results_SEResNeXt50.to_csv('Results/results_SEResNeXt50.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd8f0ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
